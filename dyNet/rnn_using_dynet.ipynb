{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This IPython Notebook is written to demonstrate a simple RNN for sentiment classification using Dynamic Neural Network (DyNet) Toolkit.  You can refer this <a href=\"https://arxiv.org/pdf/1701.03980.pdf\">arXiv Paper</a> to understand the architecture behind the framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Import Necessary Libraries </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dynet as dy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "import time\n",
    "import nltk\n",
    "import random\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Import Data </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 1)\n",
      "(25000, 1)\n",
      "                                                   0\n",
      "0  bromwell high is a cartoon comedy . it ran at ...\n",
      "1  story of a man who has unnatural feelings for ...\n",
      "2  homelessness  or houselessness as george carli...\n",
      "3  airport    starts as a brand new luxury    pla...\n",
      "4  brilliant over  acting by lesley ann warren . ...\n"
     ]
    }
   ],
   "source": [
    "reviews = pd.read_csv(\"../data/reviews.txt\",header=None)\n",
    "labels = pd.read_csv(\"../data/labels.txt\",header=None)\n",
    "word2int = defaultdict(lambda: len(word2int))\n",
    "labels2int = defaultdict(lambda: len(labels2int))\n",
    "print (reviews.shape)\n",
    "print (labels.shape)\n",
    "print (reviews.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Preprocess and Encode Data </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 0, 1, 15, 16, 17, 18, 9, 19, 20, 21, 22, 23, 24, 25, 26, 9, 27, 28, 29, 30, 31, 7, 32, 23, 33, 34, 35, 36, 37, 38, 7, 39, 40, 1, 41, 42, 43, 44, 45, 9, 35, 46, 0, 1, 47, 48, 49, 50, 51, 0, 1, 52, 53, 54]\n"
     ]
    }
   ],
   "source": [
    "stopwords = stopwords.words('english')\n",
    "punctuations = string.punctuation\n",
    "def clean(data):\n",
    "    new_data = []\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    for doc in data:\n",
    "        tokens = [tok.strip().lower() for tok in doc[0].split(' ')]\n",
    "        tokens = [tok for tok in tokens if tok not in stopwords and tok not in punctuations]\n",
    "        tokens = [wordnet_lemmatizer.lemmatize(token,pos='v') for token in tokens]\n",
    "        tokens = [word2int[tok] for tok in tokens]\n",
    "        new_data.append(tokens)\n",
    "    return new_data\n",
    "reviews = clean(reviews.values)\n",
    "labels = [labels2int[label[0]] for label in labels.values]\n",
    "print (reviews[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Split Data as Train, Dev and Test </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train reviews : 16000\n",
      "Train labels : 16000\n",
      "Dev reviews : 4000\n",
      "Dev labels : 4000\n",
      "Test reviews : 5000\n",
      "Test labels : 5000\n"
     ]
    }
   ],
   "source": [
    "nwords = len(word2int)\n",
    "nlabels = len(labels2int)\n",
    "train_x, test_x = reviews[0:int(0.8*len(reviews))],reviews[int(0.8*len(reviews)):len(reviews)]\n",
    "train_y, test_y = labels[0:int(0.8*len(labels))],labels[int(0.8*len(labels)):len(labels)]\n",
    "train_x, dev_x = train_x[0:int(0.8*len(train_x))],train_x[int(0.8*len(train_x)):len(train_x)]\n",
    "train_y, dev_y = train_y[0:int(0.8*len(train_y))],train_y[int(0.8*len(train_y)):len(train_y)]\n",
    "\n",
    "print (\"Train reviews : %d\" %(len(train_x)))\n",
    "print (\"Train labels : %d\" %(len(train_y)))\n",
    "print (\"Dev reviews : %d\" %(len(dev_x)))\n",
    "print (\"Dev labels : %d\" %(len(dev_y)))\n",
    "print (\"Test reviews : %d\" %(len(test_x)))\n",
    "print (\"Test labels : %d\" %(len(test_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Define Model and Hyperparameters </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = dy.Model()\n",
    "trainer = dy.AdamTrainer(model)\n",
    "\n",
    "EMBED_DIM = 50\n",
    "HIDDEN_DIM = 50\n",
    "Word_emb = model.add_lookup_parameters((nwords, EMBED_DIM))\n",
    "RNN = dy.SimpleRNNBuilder(1, EMBED_DIM, HIDDEN_DIM, model)\n",
    "W_sm = model.add_parameters((nlabels, HIDDEN_DIM))\n",
    "b_sm = model.add_parameters((nlabels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Define Feedforward Manipulation </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(words):\n",
    "    dy.renew_cg()\n",
    "    word_embs = [dy.lookup(Word_emb, x) for x in words]\n",
    "    rnn_init = RNN.initial_state()\n",
    "    act_embs = rnn_init.transduce(word_embs)\n",
    "    W_sm_exp = dy.parameter(W_sm)\n",
    "    b_sm_exp = dy.parameter(b_sm)\n",
    "    return W_sm_exp * act_embs[-1] + b_sm_exp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Train and Validate the Model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dy.parameter(...) call is now DEPRECATED.\n",
      "        There is no longer need to explicitly add parameters to the computation graph.\n",
      "        Any used parameter will be added automatically.\n",
      "iter 0: train loss/sent=0.62, time=25.26s\n",
      "iter 0: dev acc=0.77\n",
      "iter 1: train loss/sent=0.47, time=23.86s\n",
      "iter 1: dev acc=0.76\n",
      "iter 2: train loss/sent=0.41, time=22.96s\n",
      "iter 2: dev acc=0.73\n",
      "iter 3: train loss/sent=0.34, time=24.13s\n",
      "iter 3: dev acc=0.73\n",
      "iter 4: train loss/sent=0.31, time=25.73s\n",
      "iter 4: dev acc=0.75\n",
      "iter 5: train loss/sent=0.27, time=26.43s\n",
      "iter 5: dev acc=0.75\n",
      "iter 6: train loss/sent=0.26, time=24.77s\n",
      "iter 6: dev acc=0.71\n",
      "iter 7: train loss/sent=0.25, time=23.74s\n",
      "iter 7: dev acc=0.73\n",
      "iter 8: train loss/sent=0.23, time=23.52s\n",
      "iter 8: dev acc=0.71\n",
      "iter 9: train loss/sent=0.23, time=22.66s\n",
      "iter 9: dev acc=0.74\n",
      "iter 10: train loss/sent=0.22, time=23.15s\n",
      "iter 10: dev acc=0.74\n",
      "iter 11: train loss/sent=0.22, time=22.91s\n",
      "iter 11: dev acc=0.71\n",
      "iter 12: train loss/sent=0.22, time=23.14s\n",
      "iter 12: dev acc=0.73\n",
      "iter 13: train loss/sent=0.21, time=23.03s\n",
      "iter 13: dev acc=0.72\n",
      "iter 14: train loss/sent=0.21, time=22.97s\n",
      "iter 14: dev acc=0.71\n"
     ]
    }
   ],
   "source": [
    "for ITER in range(15):\n",
    "    train = list(zip(train_x, train_y))\n",
    "    random.shuffle(train)\n",
    "    train_x, train_y = zip(*train)\n",
    "    train_loss = 0.0\n",
    "    start = time.time()\n",
    "    for index in range(len(train_x)):\n",
    "        m_loss = dy.pickneglogsoftmax(feed_forward(train_x[index]), train_y[index])\n",
    "        train_loss += m_loss.value()\n",
    "        m_loss.backward()\n",
    "        trainer.update()\n",
    "    print(\"iter %r: train loss/sent=%.2f, time=%.2fs\" % (ITER, train_loss / len(train_x), time.time() - start))\n",
    "    \n",
    "    dev_correct = 0.0\n",
    "    for index in range(len(dev_x)):\n",
    "        scores = feed_forward(dev_x[index])\n",
    "        predict = np.argmax(scores.value())\n",
    "        if predict == dev_y[index]:\n",
    "            dev_correct += 1\n",
    "    print(\"iter %r: dev acc=%.2f\" % (ITER, dev_correct / len(dev_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Test the Model </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test acc=0.72\n"
     ]
    }
   ],
   "source": [
    "test_correct = 0.0\n",
    "for index in range(len(test_x)):\n",
    "    scores = feed_forward(test_x[index])\n",
    "    predict = np.argmax(scores.value())\n",
    "    if predict == test_y[index]:\n",
    "        test_correct += 1\n",
    "print(\"Test acc=%.2f\" % ( test_correct / len(test_y)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
